name: Schedule Notice
on:
  workflow_dispatch:
  schedule:
    - cron: "0 0-12 * * *"
permissions:
  contents: read
  pull-requests: write
  actions: read
  checks: write
  issues: read
  deployments: write
env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
  SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x # Python 버전 선택
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run Scraper
        run: python -m src.notice # 크롤링을 실행하는 스크립트 파일의 이름
      - name: action-slack
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          author_name: Connect-GNU Web Scraping
          fields: repo, action, eventName, job, workflow, took # action,eventName,ref,workflow,job,took 추가할 수 있음
          mention: here
          if_mention: failure,cancelled
          channel: "#web-scraping-notification"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} # required
        if: always() # Pick up events even if the job fails or is canceled.
